# 一致性哈希

[分布式算法 - 一致性Hash算法](https://www.pdai.tech/md/algorithm/alg-domain-distribute-x-consistency-hash.html)

## 一、普通Hash算法的问题

```java
hash(object) % nodeTotal
```

1. 一个缓存服务器宕机了，这样所有映射到这台服务器的对象都会失效，我们需要把属于该服务器中的缓存移除，这时候缓存服务器是 N-1 台，映射公式变成了 hash(object)%(N-1) ；
2. 由于QPS升高，我们需要添加多一台服务器，这时候服务器是 N+1 台，映射公式变成了 hash(object)%(N+1) 。

**1 和 2 的改变都会出现所有服务器需要进行数据迁移**。

> 这就意味着哈希表的每次扩展和收缩都会导致所有条目分布的重新计算，这个特性在某些场景下是不可接受的。比如分布式的存储系统，每个桶就相当于一个机器，文件分布在哪台机器由[哈希算法](https://www.zhihu.com/search?q=哈希算法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A24440059})来决定，这个系统想要加一台机器时就需要停下来等所有文件重新分布一次才能对外提供服务，而当一台机器掉线的时候尽管只掉了一部分数据，但所有数据访问路由都会出问题。这样整个服务就无法平滑的扩缩容，成为了有状态的服务。

## 二、一致性Hash

### 2.1 简介

一致性哈希算法在1997年由麻省理工学院的Karger等人在解决**分布式Cache**中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。

### 2.2 性质

#### 平衡性

平衡性是指哈希的结果能够尽可能分布到所有的缓存中，这样可以使得所有的缓存空间都得到利用。很多哈希算法都能够满足这一条件。

#### 单调性

单调性是指如果已经有一些内容通过了哈希分派到了相应的缓存中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。

简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：`x = (ax + b) mod (P)`，在上式中，`P`表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从`P1`到`P2`)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。

#### **分散性(Spread)**

在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于**不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中**。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。

#### **负载(Load)**

负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

#### **平滑性(Smoothness)**

平滑性是指**缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的**。

### 2.3 原理

#### Hash环

使用常见的hash算法可以把一个key值哈希到一个具有2^32个桶的空间中。也可以理解成，将key值哈希到 [0, 2^32) 的一个数字空间中。 我们假设这个是个首尾连接的环形空间。如下图:

![img](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209172154.jpeg)

假设我们现在有key1,key2,key3,key4 4个key值，我们通过一定的hash算法，将其对应到上面的环形hash空间中。

```bash
k1=hash(key1);
k2=hash(key2);
k3=hash(key3);
k4=hash(key4);
```

![img](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209172237.jpeg)



同样的，假设我们有3台cache服务器，把缓存服务器通过hash算法，加入到上述的环中。一般情况下是根据机器的IP地址或者唯一的计算机别名进行哈希。

```bash
c1=hash(cache1);
c2=hash(cache2);
c3=hash(cache3);
```

![img](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209172356.jpeg)

接下来就是数据如何存储到cache服务器上了，key值哈希之后的结果顺时针找上述环形hash空间中，距离自己最近的机器节点，然后将数据存储到上面， 如上图所示，k1 存储到 c3 服务器上， k4,k3存储到c1服务器上， k2存储在c2服务器上。用图表示如下:

![img](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209172458.jpeg)

#### 删除节点

假设cache3服务器宕机，这时候需要从集群中将其摘除。那么，之前存储再c3上的k1，将会顺时针寻找距离它最近的一个节点，也就是c1节点，这样，k1就会存储到c1上了，看一看下下面的图，比较清晰。

![img](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209172541.jpeg)

摘除c3节点之后，只影响到了原先存储再c3上的k1，而k3、k4、k2都没有受到影响，也就意味着解决了最开始的解决方案(hash(key)%N)中可能带来的雪崩问题。

#### 增加节点

新增C4节点之后，原先存储到C1的k4，迁移到了C4，分担了C1上的存储压力和流量压力。

![img](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209172628.jpeg)

#### 不平衡的问题

上面的简单的一致性hash的方案在某些情况下但依旧存在问题: 一个节点宕机之后，数据需要落到距离他最近的节点上，会导致下个节点的压力突然增大，可能导致雪崩，整个服务挂掉。

如下图所示:

![img](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209172653.jpeg)

当节点C3摘除之后，之前再C3上的k1就要迁移到C1上，这时候带来了两部分的压力:

- 之前请求到C3上的流量转嫁到了C1上,会导致C1的流量增加，如果之前C3上存在热点数据，则可能导致C1扛不住压力挂掉。
- 之前存储到C3上的key值转义到了C1，会导致C1的内容占用量增加，可能存在瓶颈。

当上面两个压力发生的时候，可能导致C1节点也宕机了。那么压力便会传递到C2上，又出现了类似滚雪球的情况，服务压力出现了雪崩，导致整个服务不可用。这一点违背了最开始提到的四个原则中的 `平衡性`， 节点宕机之后，流量及内存的分配方式打破了原有的平衡。

#### 虚拟节点

“虚拟节点”( virtual node )是实际节点(机器)在 hash 空间的复制品( replica )，一实际个节点(机器)对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。

依旧用图片来解释，假设存在以下的真实节点和虚拟节点的对应关系。

```bash
Visual100—> Real1
Visual101—> Real1
Visual200—> Real2
Visual201—> Real2
Visual300—> Real3
Visual301—> Real3
```

同样的，hash之后的结果如下:

```bash
hash(Visual100)—> V100  —> Real1
hash(Visual101)—> V101  —> Real1
hash(Visual200)—> V200  —> Real2
hash(Visual201)—> V201  —> Real2
hash(Visual300)—> V300  —> Real3
hash(Visual301)—> V301  —> Real3
```

key值的hash结果如上，这里暂时不写了。

![img](https://www.pdai.tech/_images/alg/alg-dist-hash-8.jpg)

和之前介绍的不添加虚拟节点的类似，主要聊下如果宕机之后的情况。

假设Real1机器宕机，则会发生一下情况。

- 原先存储在虚拟节点V100上的k1数据将迁移到V301上，也就意味着迁移到了Real3机器上。
- 原先存储再虚拟节点V101上的k4数据将迁移到V200上，也就意味着迁移到了Real2机器上。

结果如下图:

<img src="https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20211209174326.png" alt="img" style="zoom:150%;" />

这个就解决之前的问题了，某个节点宕机之后，存储及流量压力并没有全部转移到某台机器上，而是分散到了多台节点上。解决了节点宕机可能存在的雪崩问题。

当物理节点多的时候，虚拟节点多，这个的雪崩可能就越小。