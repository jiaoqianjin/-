# 注册中心

## 一、什么是注册中心

### 1. 注册中心

实现服务自动注册和发现，无需人为记录服务地址

### 2. 注册中心解决了什么问题

> 1. 解决了服务之间的自动发现
>
>    无注册中心时，服务间调用需要知道被调用方的地址，当服务更换部署地址后，就需要修改调用者的地址。人为的管理地址，不仅开发困难，将来测试、发布上线都非常麻烦。而注册中心的存在，使得服务间调用的时候只需知道服务名称，服务地址会通过注册中心同步过来。

## 二、四种注册中心对比

### 1. Eureka 

#### 1. 服务治理

问题分析

>provider对外提供服务，需要对外暴露自己的地址。而consumer（调用者）需要记录服务提供者的地址。将来地址出现变更，还需要及时更新。这在服务较少的时候并不觉得有什么，但是在现在日益复杂的互联网环境，一个项目肯定会拆分出十几，甚至数十个微服务。此时如果还人为管理地址，不仅开发困难，将来测试、发布上线都会非常麻烦，这与DevOps的思想是背道而驰的。

Spring Cloud封装了Netflix 公司开发的Eureka模块来实现服务治理

在传统的RPC远程调用框架中，管理每个服务与服务之间依赖关系比较复杂，管理比较复杂，所以需要使用服务治理，管理服务于服务之间依赖关系，可以实现服务调用、负载均衡、容错等，实现服务发现与注册。

#### 2. 服务注册与发现

Eureka采用了CS的设计架构，Eureka Sever作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务，使用Eureka的客户端连接到 Eureka Server并维持心跳连接。这样系统的维护人员就可以通过Eureka Server来监控系统中各个微服务是否正常运行。

在服务注册与发现中，有一个注册中心。当服务器启动的时候，会把当前自己服务器的信息比如服务地址通讯地址等以别名方式注册到注册中心上。另一方(消费者服务提供者)，以该别名的方式去注册中心上获取到实际的服务通讯地址，然后再实现本地RPC调用RPC远程调用框架核心设计思想:在于注册中心，因为使用注册中心管理每个服务与服务之间的一个依赖关系(服务治理概念)。在任何RPC远程框架中，都会有一个注册中心存放服务地址相关信息(接口地址)

#### 3. 原理图

![1525597885059](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20210706105522.png) 

- Eureka：就是服务注册中心（可以是一个集群），对外暴露自己的地址
- 提供者：启动后向Eureka注册自己信息（地址，提供什么服务）
- 消费者：向Eureka订阅服务，Eureka会将对应服务的所有提供者地址列表发送给消费者，并且定期更新
- 心跳(续约)：提供者定期通过http方式向Eureka刷新自己的状态

#### 4. Eureka详解

##### 2.1.4.1. 基础架构

​	Eureka架构中的三个核心角色：

- 服务注册中心

  Eureka的服务端应用，提供服务注册和发现功能

- 服务提供者

  提供服务的应用，可以是SpringBoot应用，也可以是其它任意技术实现，只要对外提供的是Rest风格服务即可。

- 服务消费者

  消费应用从注册中心获取服务列表，从而得知每个服务方的信息，知道去哪里调用服务方。

##### 2.1.4.2. 服务提供者

​	服务提供者要向EurekaServer注册服务，并且完成服务续约等工作。

**服务注册**

​		服务提供者在启动时，会检测配置属性中的：`eureka.client.register-with-eureka=true`参数是否正确，事实上默认就是true。如果值确实为true，则会向EurekaServer发起一个Rest请求，并携带自己的元数据信息，Eureka Server会把这些信息保存到一个双层Map结构中。

- 第一层Map的Key就是服务id，一般是配置中的`spring.application.name`属性
- 第二层Map的key是服务的实例id。一般host+ serviceId + port，例如：`locahost:service-provider:8081`
- 值则是服务的实例对象，也就是说一个服务，可以同时启动多个不同实例，形成集群。

**服务续约**

在注册服务完成以后，服务提供者会维持一个心跳（定时向EurekaServer发起Rest请求），告诉EurekaServer：“我还活着”。这个我们称为服务的续约（renew）；

有两个重要参数可以修改服务续约的行为：

```yaml
eureka:
  instance:
    lease-expiration-duration-in-seconds: 90
    lease-renewal-interval-in-seconds: 30
```

- lease-renewal-interval-in-seconds：服务续约(renew)的间隔，默认为30秒
- lease-expiration-duration-in-seconds：服务失效时间，默认值90秒

也就是说，默认情况下每个30秒服务会向注册中心发送一次心跳，证明自己还活着。如果超过90秒没有发送心跳，EurekaServer就会认为该服务宕机，会从服务列表中移除，这两个值在生产环境不要修改，默认即可。

但是在开发时，这个值有点太长了，经常我们关掉一个服务，会发现Eureka依然认为服务在活着。所以我们在开发阶段可以适当调小。

```yaml
eureka:
  instance:
    lease-expiration-duration-in-seconds: 10 # 10秒即过期
    lease-renewal-interval-in-seconds: 5 # 5秒一次心跳
```

##### 2.1.4.3. 服务消费者

**获取服务列表**

当服务消费者启动时，会检测`eureka.client.fetch-registry=true`参数的值，如果为true，则会拉取Eureka Server服务的列表只读备份，然后缓存在本地。并且`每隔30秒`会重新获取并更新数据。我们可以通过下面的参数来修改：

```yaml
eureka:
  client:
    registry-fetch-interval-seconds: 5
```

生产环境中，我们不需要修改这个值。

但是为了开发环境下，能够快速得到服务的最新状态，我们可以将其设置小一点。



##### 2.1.4.4. 失效剔除和自我保护

**服务下线**

当服务进行正常关闭操作时，它会触发一个服务下线的REST请求给Eureka Server，告诉服务注册中心：“我要下线了”。服务中心接受到请求之后，将该服务置为下线状态。

**失效剔除**

有些时候，我们的服务提供方并不一定会正常下线，可能因为内存溢出、网络故障等原因导致服务无法正常工作。Eureka Server需要将这样的服务剔除出服务列表。因此它会开启一个定时任务，每隔60秒对所有失效的服务（超过90秒未响应）进行剔除。

可以通过`eureka.server.eviction-interval-timer-in-ms`参数对其进行修改，单位是毫秒，生产环境不要修改。

这个会对我们开发带来极大的不变，你对服务重启，隔了60秒Eureka才反应过来。开发阶段可以适当调整，比如：10秒

![1528696142799](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20210706111038.png)



**自我保护**

我们关停一个服务，就会在Eureka面板看到一条警告：

![1525618396076](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20210706111038.png)

这是触发了Eureka的自我保护机制。当一个服务未按时进行心跳续约时，Eureka会统计最近15分钟心跳失败的服务实例的比例是否超过了85%。在生产环境下，因为网络延迟等原因，心跳失败实例的比例很有可能超标，但是此时就把服务剔除列表并不妥当，因为服务可能没有宕机。Eureka就会把当前实例的注册信息保护起来，不予剔除。生产环境下这很有效，保证了大多数服务依然可用。

但是这给我们的开发带来了麻烦， 因此开发阶段我们都会关闭自我保护模式：

```yaml
eureka:
  server:
    enable-self-preservation: false # 关闭自我保护模式（缺省为打开）
    eviction-interval-timer-in-ms: 1000 # 扫描失效服务的间隔时间（缺省为60*1000ms）
```

### 2. Zookeeper

### 3. Consul

### 4. Nacos

### 5. CAP理论

> C、一致性（Consistency）
>
> ​	所有节点在同一时间具有相同的数据
>
> A、可用性（Availability）
>
> ​	保证每个请求不管成功或者失败都有响应
>
> P、分区容错性（Partition tolerance）
>
> ​	系统中任意信息的丢失或失败不会影响系统的继续运作

**CAP原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾**

**分析：**

> ​		如果 C（一致性）是第一需求，那么A（可用性）的性能就不能保证。因为在数据同步保持请求结果相同的时候，或消耗时间，这时可用性就会降低，在数据同步的时候，不能保证能正常接收请求。
>
> ​		如果 A （可用性）是第一需求，那么 C (一致性) 就不能保证。只要有一个服务在，就能正常接受请求，但是对与返回结果变不能保证，原因是，在分布式部署的时候，数据一致的过程不可能想切线路那么快。
>
> ​		如果同时满足一致性和可用性，那么分区容错就很难保证了。

![image-20210702141248807](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20210702141248.png)

![image-20210702141304178](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20210702141304.png)



### 6. 对比

[主流注册中心ZooKeeper、Eureka、Consul 、Nacos对比](https://www.pianshen.com/article/55361043112/)

[微服务注册中心Eureka、zooKeeper、consul、Nacos对比](https://www.pianshen.com/article/56141778477/)

![3c2c909dcd5b9301006af1357908358e.png](https://gitee.com/jiao_qianjin/zhishiku/raw/master/img/20210708155252.png)

#### 1. Eureka(AP)

Eureka强调了CAP理论中的AP，即可用性和可靠性，一致性方面保证的是弱一致性，只能保证最终的一致性。

> Eureka为了保证高可用，在任何时候，服务消费者都能正常获取服务列表，但不保证数据的强一致性，消费者可能会拿到过期的服务列表。Eureka为保证其弱一致性，在数据同步时采用的是**对等复制**（Peer to Peer）。副本间不分主从，任何副本都可以接收写操作，然后每个副本间互相进行数据更新。对等复制模式，任何副本都可以接收写请求，不存在写压力瓶颈，但各个副本间数据同步时可能产生数据冲突。
>
> *与之相对的是**主从复制**，**Master-Slave** 模式，有一个主副本，其他为从副本，所有写操作都提交到主副本，再由主副本更新到其他从副本。写压力都集中在主副本上，是系统的瓶颈，从副本可以分担读请求。*

#### 2. ZooKeeper(CP)

> ZooKeeper强调了CAP理论中的CP，即一致性与可靠性，任何时候对 Zookeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性，但是 Zookeeper 不能保证每次服务请求都是可达的。
>
> 从 Zookeeper 的实际应用情况来看，在使用 Zookeeper 获取服务列表时，如果此时的 Zookeeper 集群中的 Leader 宕机了，该集群就要进行 Leader 的选举，又或者 Zookeeper 集群中半数以上服务器节点不可用（例如有三个节点，如果节点一检测到节点三挂了 ，节点二也检测到节点三挂了，那这个节点才算是真的挂了），那么将无法处理该请求。所以说，Zookeeper 不能保证服务可用性。
>
> 当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30~120s，而且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署环境下， 因为网络问题使得zk集群失去master节点是大概率事件，虽然服务能最终恢复，但是漫长的选举事件导致注册长期不可用是不能容忍的。
>
> 当然，在大多数分布式环境中，尤其是涉及到数据存储的场景，数据一致性应该是首先被保证的，这也是 Zookeeper 设计紧遵CP原则的另一个原因。
>
> 但是对于服务发现来说，情况就不太一样了，针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。
>
> 因为对于服务消费者来说，能消费才是最重要的，消费者虽然拿到可能不正确的服务实例信息后尝试消费一下，也要胜过因为无法获取实例信息而不去消费，导致系统异常要好（淘宝的双十一，京东的618就是紧遵AP的最好参照）。

#### 3. Consul(CP)

> Consul 遵循CAP原理中的CP，保证了强一致性和分区容错性，且使用的是Raft算法，比zookeeper使用的Paxos算法更加简单。虽然保证了强一致性，但是可用性就相应下降了，例如服务注册的时间会稍长一些，因为 Consul 的 raft 协议要求必须过半数的节点都写入成功才认为注册成功 ；在leader挂掉了之后，重新选举出leader之前会导致Consul 服务不可用。

#### 4. Nacos(CP+AP)

> Nacos是阿里开源的，Nacos 支持基于 DNS 和基于 RPC 的服务发现。在Spring Cloud中使用Nacos，只需要先下载 Nacos 并启动 Nacos server，Nacos只需要简单的配置就可以完成服务的注册发现。
>
> Nacos除了服务的注册发现之外，还支持动态配置服务。动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。
>
> 一句话概括就是Nacos = Spring Cloud注册中心 + Spring Cloud配置中心。

#### 5. 使用区别

> ### 性能和容量
>
> Eureka在实例数达到5000左右时，就会出现服务不可用的问题。而Zookeeper和Nacos能够达到百万级，这部分的性能不是说越高越好，提高这些性能都是在其它方面做出了牺牲的。
>
> ### 易用性
>
> 易用性包括多方面的工作，例如API和客户端的接入是否简单，文档是否齐全易懂，控制台界面是否完善等。对于开源产品来说，还有一块是社区是否活跃。Zookeeper的易用性是比较差的，Zookeeper的客户端使用比较复杂，没有针对服务发现的模型设计以及相应的API封装，需要依赖方自己处理。对多语言的支持也不太好，同时没有比较好用的控制台进行运维管理。
>
> 从目前来看（2020年初），Eureka和Nacos有针对服务注册与发现的客户端，也有基于SpringCloud体系的starter，帮助用户以非常低的成本无感知的做到服务注册与发现。同时还暴露标准的HTTP接口，支持多语言和跨平台访问。从社区活跃度的角度来看，目前由于Zookeeper和Eureka的存量用户较多，很多教程以及问题排查都可以在社区搜索到，这方面新开源的Nacos还需要随着时间继续沉淀。综合考虑，Eureka的易用性最高。
>
> ### 集群扩展性
>
> 集群扩展性和集群容量以及读写性能关系紧密。当使用一个比较小的集群规模就可以支撑远高于现有数量的服务注册及访问时，集群的扩展能力暂时就不会那么重要。从协议的层面上来说，Zookeeper使用的ZAB协议，由于是单点写，在集群扩展性上不具备优势。Eureka在协议上来说理论上可以扩展到很大规模，因为都是点对点的数据同步，但是从我们对Eureka的运维经验来看，Eureka集群在扩容之后，性能上有很大问题。

